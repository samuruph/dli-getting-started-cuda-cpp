WEBVTT

00:00:00.891 --> 00:00:04.352
Atomics let us regain functional
correctness, but this comes

00:00:04.432 --> 00:00:07.639
at a cost of serialization
of memory accesses.

00:00:07.714 --> 00:00:10.495
While they solve the data
race problem, they can't run

00:00:10.555 --> 00:00:14.636
in parallel when all threads
target the same memory location.

00:00:14.716 --> 00:00:18.482
Instead, each atomic gets
queued up one after the other.

00:00:18.558 --> 00:00:22.685
We launch about 256 threads
and about 16,000 blocks.

00:00:22.740 --> 00:00:27.121
In total, this gives us a queue
of 4 million atomic operations,

00:00:27.181 --> 00:00:29.299
which obviously is too much.

00:00:29.346 --> 00:00:32.106
But what are our options?

00:00:32.249 --> 00:00:36.314
One common solution to this
contention is called privatization.

00:00:36.393 --> 00:00:40.791
Instead of having all threads
update the same global histogram,

00:00:40.838 --> 00:00:45.870
we allocate a small private
histogram for each thread block.

00:00:45.943 --> 00:00:49.402
The threads within a block
can then safely use Atomics

00:00:49.427 --> 00:00:51.966
on their private histogram.

00:00:52.111 --> 00:00:55.316
Once they're done, the block
aggregates its counts and

00:00:55.332 --> 00:01:00.354
updates the main histogram
with just one atomic operation.

00:01:00.455 --> 00:01:03.224
In our simple two-block example,
each block increments its

00:01:03.236 --> 00:01:06.597
own private histogram to a
total of two and then adds that

00:01:06.657 --> 00:01:09.550
two to the global histogram once.

00:01:09.638 --> 00:01:12.646
Although that might not help
much with just two blocks,

00:01:12.680 --> 00:01:15.296
imagine having 16,000 of them.

00:01:15.381 --> 00:01:19.034
In this scenario, we'd have
16,000 private histograms

00:01:19.062 --> 00:01:20.936
updated in parallel.

00:01:20.998 --> 00:01:24.890
Followed by just 16,000
atomic operations.

00:01:25.001 --> 00:01:28.532
This is far fewer than the 4
million we'd otherwise have

00:01:28.544 --> 00:01:32.654
if every thread incremented
the final histogram directly.

00:01:32.787 --> 00:01:36.470
To implement this privatization,
we add a new parameter to

00:01:36.530 --> 00:01:39.569
our kernel called block histograms.

00:01:39.692 --> 00:01:43.481
Next we create a subspan for the
current thread blocks portion

00:01:43.495 --> 00:01:45.645
of the privatized histogram.

00:01:45.717 --> 00:01:49.930
The subspan function takes
an offset and a size.

00:01:49.994 --> 00:01:53.275
We calculate the offset by
multiplying block index by

00:01:53.315 --> 00:01:55.037
the histogram size.

00:01:55.115 --> 00:01:58.316
So the first block has an offset
of zero and the second block

00:01:58.336 --> 00:02:00.735
has an offset of 10 and so on.

00:02:00.797 --> 00:02:04.985
This ensures each block writes
to its own slice of memory.

00:02:05.078 --> 00:02:08.510
Then, just as before, each
thread computes the bin index.

00:02:08.559 --> 00:02:11.409
However, we now increment the
privatized block histogram

00:02:11.419 --> 00:02:13.394
instead of the global one.

00:02:13.480 --> 00:02:16.601
After that, each block adds
its partial counts back into

00:02:16.641 --> 00:02:19.352
the global histogram in parallel.

00:02:19.454 --> 00:02:22.895
In the code snippet, the first
10 threads in the block each

00:02:22.975 --> 00:02:26.435
handle one bin from the block's
private histogram.

00:02:26.515 --> 00:02:29.996
They use an atomic reference
to safely add their local

00:02:30.056 --> 00:02:33.386
counts to the final histogram.

00:02:33.557 --> 00:02:36.991
Unfortunately, the highlighted
code has a bug in it.

00:02:37.058 --> 00:02:39.552
Let's take a closer look.

00:02:39.698 --> 00:02:42.859
In this code, we've assumed
that all threads finish updating

00:02:42.899 --> 00:02:46.723
the block histogram before
the next section of code begins.

00:02:46.759 --> 00:02:49.533
However, CUDA threads in a
block run concurrently

00:02:49.571 --> 00:02:51.850
without guaranteed ordering.

00:02:51.933 --> 00:02:54.934
This means that some threads
might have not even started

00:02:55.015 --> 00:02:59.189
running yet, while others
have already exited the kernel.

00:02:59.317 --> 00:03:02.459
One of the possible scenarios
is that some thread might

00:03:02.499 --> 00:03:05.971
have read the block histogram
before it was updated.

00:03:06.022 --> 00:03:09.048
To fix this issue, we have
to somehow make sure that

00:03:09.064 --> 00:03:14.226
all threads finished updating the
block histogram before we read it.

00:03:14.387 --> 00:03:18.066
CUDA provides a special function
called SyncThreads.

00:03:18.160 --> 00:03:21.376
It acts as a barrier for all
threads in a block.

00:03:21.443 --> 00:03:25.594
When a thread reaches this barrier,
it waits until every other thread

00:03:25.606 --> 00:03:28.123
in the same block has arrived.

00:03:28.188 --> 00:03:31.971
In that sense, it's similar
to STD barrier, but there's

00:03:32.071 --> 00:03:33.635
one important difference.

00:03:33.712 --> 00:03:36.634
Every thread in the block
must call sync threads from

00:03:36.674 --> 00:03:39.097
the same logical path.

00:03:39.238 --> 00:03:41.561
Which brings us to
the next exercise.

00:03:41.604 --> 00:03:44.972
This time you'll have to use
thread block synchronization to

00:03:45.032 --> 00:03:48.802
make sure all threads updated the
block histogram before reading it.

