{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/nvidia_header.png\" style=\"margin-left: -30px; width: 300px; float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlocking the GPUâ€™s Full Potential: Summary\n",
    "\n",
    "Congratulations on completing the second part of the course!  You have accomplished all of the following lab objectives:\n",
    "- Learn the notion of asynchrony to take advantage of idle CPU time\n",
    "- Appy CUB algorithms to leverage asynchrony \n",
    "- Use [NVIDIA Nsight Systems](https://developer.nvidia.com/nsight-systems) and NVTX to profile your code and identify performance bottlenecks. \n",
    "- Define CUDA Streams to further take advantage of asynchronous programming\n",
    "- Allocate pinned memory directly to facilitate asynchrony that is otherwise blocked "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "Let's review a few key points from this part of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prefer asynchronous programming model to overlap memory transfers, CPU tasks, and GPU computations.\n",
    "\n",
    "<img src=\"Images/takeaway_2_1.png\" alt=\"Asynchronous\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use CUB for general-purpose asynchronous parallel algorithms.\n",
    "\n",
    "<img src=\"Images/takeaway_2_2.png\" alt=\"CUB\" width=600>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Nsight to profile your code, identify opportunities for optimization, and visualize asynchronous execution.\n",
    "\n",
    "<img src=\"Images/takeaway_2_3.png\" alt=\"Nsight\" width=600>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use CUDA streams to enable asynchronous execution with CPU and other streams\n",
    "\n",
    "<img src=\"Images/takeaway_2_4.png\" alt=\"CUDA Streams\" width=600>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `cudaMemcpyAsync` to make copies asynchronous with respect to CPU.\n",
    "\n",
    "<img src=\"Images/takeaway_2_5.png\" alt=\"cudaMemcpyAsync\" width=600>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pinned memory for asynchronous data transfers and improved performance.\n",
    "\n",
    "<img src=\"Images/takeaway_2_6.png\" alt=\"pinned memory\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "To go to the next lab, open the [CUDA Kernels introduction](../03.01-Introduction/03.01-Introduction.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/nvidia_header.png\" style=\"margin-left: -30px; width: 300px; float: left;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
