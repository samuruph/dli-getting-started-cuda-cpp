{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/nvidia_header.png\" style=\"margin-left: -30px; width: 300px; float: left;\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA Made Easy: Summary\n",
    "\n",
    "Congratulations on completing the first part of the course!  You have accomplished all of the following lab objectives:\n",
    "\n",
    "- Understand the differences between parallel and serial algorithms\n",
    "- Control where your C++ code runs and run it on CPU or GPU\n",
    "- Control whether your data resides in CPU or GPU memory and how to move between them \n",
    "- Refactor standard algorithms to execute on GPU\n",
    "- Leverage powerful parallel algorithms that simplify adding GPU acceleration to your code\n",
    "- Use fancy iterators to fuse operations and extend algorithms to fit your unique use cases\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "Let's review a few key points from this part of the course."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use NVCC to compile your code for NVIDIA GPUs.\n",
    "\n",
    "<img src=\"Images/takeaway_1_1.png\" alt=\"kernels\" width=600>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leverage accelerated libraries whenever you can.\n",
    "Existing libraries are already tuned to make the most of GPU hardware. \n",
    "For example, use Thrust for general-purpose parallel algorithms and container management.\n",
    "Use cuSPARSE when you need GPU-accelerated sparse linear algebra functions.\n",
    "Use MatX for array-based numerical computing, etc.\n",
    "\n",
    "<img src=\"Images/takeaway_1_2.png\" alt=\"syncthreads\" width=600>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When writing your CUDA code, remember the difference between execution specifiers and policies:\n",
    "Use __host__ __device__ execution space specifiers to specify where given function could be executed.\n",
    "Use thrust::host and thrust::device execution policies to specify where given thrust algorithm will be executed.\n",
    "\n",
    "<img src=\"Images/takeaway_1_3.png\" alt=\"shared\" width=600>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoid serialization as much as possible.\n",
    "\n",
    "<img src=\"Images/takeaway_1_4.png\" alt=\"libs\" width=600>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where possible, use explicit memory spaces to avoid unexpected slowdowns. \n",
    "Rather than relying on implicit managed memory transfers, define separate host and device vectors and manage data movement yourself. \n",
    "This way, you’re always in control of when and how data travels between the CPU and GPU.\n",
    "\n",
    "<img src=\"Images/takeaway_1_5.png\" alt=\"dev tools\" width=600>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And lastly, fancy iterators give you a powerful way to extend existing parallel algorithms to your unique use cases. \n",
    "By encapsulating custom logic, you can reuse Thrust’s highly optimized building blocks without having to rewrite entire algorithms from scratch.\n",
    "\n",
    "<img src=\"Images/takeaway_1_6.png\" alt=\"libs\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "If you are ready to move on, proceed to the next part, [Unlocking the GPU’s Full Potential: Asynchrony and CUDA Streams](../02.01-Introduction/02.01.01-Introduction.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/nvidia_header.png\" style=\"margin-left: -30px; width: 300px; float: left;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
