{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/nvidia_header.png\" style=\"margin-left: -30px; width: 300px; float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shared Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "* [Cache Memory](#Cache-Memory)\n",
    "* [Exercise: Optimize Histogram](03.05.02-Exercise-Optimize-Histogram.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video controls width=\"800\">\n",
       "  <source src=\"https://d36m44n9vdbmda.cloudfront.net/assets/s-ac-04-v2/videos/0305_SharedMem.mp4\" type=\"video/mp4\">\n",
       "  <track src=\"Images/transcript_0305.vtt\" \n",
       "        kind=\"subtitles\" srclang=\"en\" label=\"English\" default>\n",
       "  Sorry, your browser does not support embedded videos.\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<video controls width=\"800\">\n",
    "  <source src=\"https://d36m44n9vdbmda.cloudfront.net/assets/s-ac-04-v2/videos/0305_SharedMem.mp4\" type=\"video/mp4\">\n",
    "  <track src=\"Images/transcript_0305.vtt\" \n",
    "        kind=\"subtitles\" srclang=\"en\" label=\"English\" default>\n",
    "  Sorry, your browser does not support embedded videos.\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "With our previous optimizations, the kernel now performs significantly better. \n",
    "However, some inefficiencies remain. \n",
    "Currently, each block’s histogram is stored in global GPU memory, even though it’s never used outside the kernel. \n",
    "This approach not only consumes unnecessary bandwidth but also increases the overall memory footprint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache Memory\n",
    "\n",
    "<img src=\"Images/L2.png\" alt=\"L2\" width=900>\n",
    "\n",
    "As shown in the figure above, there’s a much closer memory resource: each Streaming Multiprocessor (SM) has its own L1 cache. \n",
    "Ideally, we want to store each block’s histogram right there in L1. \n",
    "Fortunately, CUDA makes this possible through software-controlled shared memory. \n",
    "By allocating the block histogram in shared memory, we can take full advantage of the SM’s L1 cache and reduce unnecessary memory traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Sources/simple-shmem.cu\n",
    "#include <cstdio>\n",
    "\n",
    "__global__ void kernel()\n",
    "{\n",
    "  __shared__ int shared[4];\n",
    "  shared[threadIdx.x] = threadIdx.x;\n",
    "  __syncthreads();\n",
    "\n",
    "  if (threadIdx.x == 0)\n",
    "  {\n",
    "    for (int i = 0; i < 4; i++) {\n",
    "      std::printf(\"shared[%d] = %d\\n\", i, shared[i]);\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "  kernel<<<1, 4>>>();\n",
    "  cudaDeviceSynchronize();\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o /tmp/a.out Sources/simple-shmem.cu && /tmp/a.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allocate shared memory, simply annotate a variable with the `__shared__` keyword.\n",
    "This puts the variable into shared memory that coresides with the L1 cache.\n",
    "Since shared memory isn't automatically initialized, \n",
    "we begin our kernel by having each thread write its own index into a corresponding shared memory location:\n",
    "\n",
    "```c++\n",
    "shared[threadIdx.x] = threadIdx.x;\n",
    "__syncthreads();\n",
    "```\n",
    "\n",
    "The `__syncthreads()` call ensures that all threads have finished writing to the shared array before any thread reads from it. \n",
    "Afterwards, the first thread prints out the contents of the shared memory:\n",
    "\n",
    "<img src=\"Images/simply-shared.png\" alt=\"Shared Memory\" width=600>\n",
    "\n",
    "As you can see, each thread successfully stored its index in the shared array, and the first thread can read back those values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now lets go ahead and try an [exercise](03.05.02-Exercise-Optimize-Histogram.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/nvidia_header.png\" style=\"margin-left: -30px; width: 300px; float: left;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
