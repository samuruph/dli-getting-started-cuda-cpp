{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/nvidia_header.png\" style=\"margin-left: -30px; width: 300px; float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Optimize Histogram\n",
    "\n",
    "You can allocate shared memory with `__shared__` memory space specifier.\n",
    "\n",
    "<img src=\"Images/shared.png\" alt= \"Shared\" width=900>\n",
    "\n",
    "Use shared memory to optimize the performance of the histogram.  You will do this algorithm in two stages:\n",
    "1. Compute a privatized histogram for each thread block.\n",
    "2. Contribute the privatized histogram to the global histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Original code in case you need it.</summary>\n",
    "\n",
    "```c++\n",
    "%%writefile Sources/shmem.cu\n",
    "#include \"dli.cuh\"\n",
    "\n",
    "constexpr int num_bins = 10;\n",
    "constexpr float bin_width = 10;\n",
    "\n",
    "// 1. Remove `block_histograms` from kernel parameters\n",
    "__global__ void histogram_kernel(cuda::std::span<float> temperatures,\n",
    "                                 cuda::std::span<int> block_histograms,\n",
    "                                 cuda::std::span<int> histogram) \n",
    "{\n",
    "  // 2. Allocate `block_histogram` in shared memory and initialize it to 0\n",
    "  cuda::std::span<int> block_histogram =\n",
    "      block_histograms.subspan(blockIdx.x * histogram.size(), histogram.size());\n",
    "\n",
    "  int cell = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  int bin = static_cast<int>(temperatures[cell] / bin_width);\n",
    "\n",
    "  cuda::atomic_ref<int, cuda::thread_scope_block> \n",
    "    block_ref(block_histogram[bin]);\n",
    "  block_ref.fetch_add(1);\n",
    "  __syncthreads();\n",
    "\n",
    "  if (threadIdx.x < num_bins) \n",
    "  {\n",
    "    cuda::atomic_ref<int, cuda::thread_scope_device> ref(histogram[threadIdx.x]);\n",
    "    ref.fetch_add(block_histogram[threadIdx.x]);\n",
    "  }\n",
    "}\n",
    "\n",
    "void histogram(cuda::std::span<float> temperatures,\n",
    "               cuda::std::span<int> block_histograms,\n",
    "               cuda::std::span<int> histogram, cudaStream_t stream) {\n",
    "  int block_size = 256;\n",
    "  int grid_size = cuda::ceil_div(temperatures.size(), block_size);\n",
    "  histogram_kernel<<<grid_size, block_size, 0, stream>>>(\n",
    "      temperatures, block_histograms, histogram);\n",
    "}\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Sources/shmem.cu\n",
    "#include \"dli.cuh\"\n",
    "\n",
    "constexpr int num_bins = 10;\n",
    "constexpr float bin_width = 10;\n",
    "\n",
    "// 1. Remove `block_histograms` from kernel parameters\n",
    "__global__ void histogram_kernel(cuda::std::span<float> temperatures,\n",
    "                                 cuda::std::span<int> block_histograms,\n",
    "                                 cuda::std::span<int> histogram) \n",
    "{\n",
    "  // 2. Allocate `block_histogram` in shared memory and initialize it to 0\n",
    "  cuda::std::span<int> block_histogram =\n",
    "      block_histograms.subspan(blockIdx.x * histogram.size(), histogram.size());\n",
    "\n",
    "  int cell = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  int bin = static_cast<int>(temperatures[cell] / bin_width);\n",
    "\n",
    "  cuda::atomic_ref<int, cuda::thread_scope_block> \n",
    "    block_ref(block_histogram[bin]);\n",
    "  block_ref.fetch_add(1);\n",
    "  __syncthreads();\n",
    "\n",
    "  if (threadIdx.x < num_bins) \n",
    "  {\n",
    "    cuda::atomic_ref<int, cuda::thread_scope_device> ref(histogram[threadIdx.x]);\n",
    "    ref.fetch_add(block_histogram[threadIdx.x]);\n",
    "  }\n",
    "}\n",
    "\n",
    "void histogram(cuda::std::span<float> temperatures,\n",
    "               cuda::std::span<int> block_histograms,\n",
    "               cuda::std::span<int> histogram, cudaStream_t stream) {\n",
    "  int block_size = 256;\n",
    "  int grid_size = cuda::ceil_div(temperatures.size(), block_size);\n",
    "  histogram_kernel<<<grid_size, block_size, 0, stream>>>(\n",
    "      temperatures, block_histograms, histogram);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Sources.dli\n",
    "Sources.dli.run(\"Sources/shmem.cu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you’re unsure how to proceed, consider expanding this section for guidance. Use the hint only after giving the problem a genuine attempt.\n",
    "\n",
    "<details>\n",
    "  <summary>Hints</summary>\n",
    "  \n",
    "  - You can allocate shared memory using the `__shared__` keyword\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open this section only after you’ve made a serious attempt at solving the problem. Once you’ve completed your solution, compare it with the reference provided here to evaluate your approach and identify any potential improvements.\n",
    "\n",
    "<details>\n",
    "  <summary>Solution</summary>\n",
    "\n",
    "  Key points:\n",
    "\n",
    "  - Allocate a shared memory array\n",
    "\n",
    "  Solution:\n",
    "  ```c++\n",
    "  __shared__ int block_histogram[num_bins];\n",
    "\n",
    "  if (threadIdx.x < num_bins) \n",
    "  {\n",
    "    block_histogram[threadIdx.x] = 0;\n",
    "  }\n",
    "  __syncthreads();\n",
    "\n",
    "  int cell = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  int bin = static_cast<int>(temperatures[cell] / bin_width);\n",
    "\n",
    "  cuda::atomic_ref<int, cuda::thread_scope_block> \n",
    "    block_ref(block_histogram[bin]);\n",
    "  block_ref.fetch_add(1, cuda::memory_order_relaxed);\n",
    "  __syncthreads();\n",
    "\n",
    "  if (threadIdx.x < num_bins) \n",
    "  {\n",
    "    cuda::atomic_ref<int, cuda::thread_scope_device> ref(histogram[threadIdx.x]);\n",
    "    ref.fetch_add(block_histogram[threadIdx.x], cuda::memory_order_relaxed);\n",
    "  }\n",
    "  ```\n",
    "\n",
    "  You can find full solution [here](Solutions/shmem.cu).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Congratulations!  Move on to the [next section](../03.06-Cooperative-Algorithms/03.06.01-Cooperative.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/nvidia_header.png\" style=\"margin-left: -30px; width: 300px; float: left;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
