WEBVTT

00:00:01.035 --> 00:00:04.678
Before optimizing our histogram
kernel even further, let's

00:00:04.698 --> 00:00:06.901
revisit what we've learned so far.

00:00:06.960 --> 00:00:09.904
We've just covered atomic
operations, thread scopes,

00:00:09.922 --> 00:00:13.469
subspans, but what about our
earlier question?

00:00:13.525 --> 00:00:17.254
Why is thread hierarchy needed
in the first place?

00:00:17.369 --> 00:00:20.447
Let's take another look at
thread block synchronization,

00:00:20.491 --> 00:00:24.379
which is only available between
threads of a given thread block.

00:00:24.451 --> 00:00:27.555
This makes different groups
of thread not equally capable,

00:00:27.573 --> 00:00:31.577
hence introducing non-uniformity
in thread grouping and thread

00:00:31.617 --> 00:00:33.884
hierarchy as a result.

00:00:33.978 --> 00:00:38.190
But are there any other resources
that are only accessible within

00:00:38.202 --> 00:00:40.339
certain groups of threads?

00:00:40.444 --> 00:00:43.730
To find out, let's examine
the GPU's architecture.

00:00:43.806 --> 00:00:47.629
GPUs are composed of uniform
building blocks called streaming

00:00:47.689 --> 00:00:50.656
multiprocessors, or SMs.

00:00:50.808 --> 00:00:53.618
A single GPU can contain
hundreds of them.

00:00:53.670 --> 00:00:58.985
Each SM has a lot of cores,
but also local L1 cache.

00:00:59.073 --> 00:01:03.066
The main GPU memory lives
outside these SMs, which is

00:01:03.076 --> 00:01:06.327
why we call it global memory.

00:01:06.458 --> 00:01:09.080
Some features, like thread
block synchronization, are

00:01:09.140 --> 00:01:11.585
built into the SM hardware.

00:01:11.641 --> 00:01:14.765
To use these SM-specific
features, thread blocks

00:01:14.783 --> 00:01:18.920
always run entirely on a single SM.

00:01:19.082 --> 00:01:23.164
Let's think about how our histogram
kernel maps to this hardware.

00:01:23.244 --> 00:01:27.726
Each thread block runs on
one SM, and block histograms

00:01:27.786 --> 00:01:30.183
are stored in global memory.

00:01:30.267 --> 00:01:34.589
Then each thread issues atomics
into its block private histogram,

00:01:34.629 --> 00:01:39.738
which likely go into L2, which
is slower cache memory shared

00:01:39.752 --> 00:01:42.275
globally across all SMs.

00:01:42.373 --> 00:01:45.034
That's not very efficient
because we don't actually need

00:01:45.074 --> 00:01:48.773
to keep Keep these block histograms
around once the kernel finishes.

00:01:48.851 --> 00:01:50.470
They're just temporary storage.

00:01:50.512 --> 00:01:54.835
Meanwhile, the L1 cache sits
right there on the SM.

00:01:54.933 --> 00:01:59.113
If we could allocate the block
histogram in L1 directly,

00:01:59.155 --> 00:02:01.942
we'd avoid wasting space
in global memory.

00:02:01.976 --> 00:02:05.038
Besides that, L1 cache
is physically closer than

00:02:05.078 --> 00:02:09.791
global memory, so we'd
likely improve performance.

00:02:09.966 --> 00:02:12.300
Fortunately, we
can do exactly that.

00:02:12.348 --> 00:02:16.745
CUDA provides a software-defended
cache that's called shared memory.

00:02:16.811 --> 00:02:20.710
Shared memory is only accessible
within a given thread block

00:02:20.754 --> 00:02:24.414
and is co-located with L1.

00:02:24.597 --> 00:02:27.460
To allocate shared memory,
we simply add the shared

00:02:27.520 --> 00:02:29.525
specifier to a variable.

00:02:29.601 --> 00:02:34.753
In this example, we declare an
array of four integers as shared.

00:02:34.886 --> 00:02:37.347
From within a thread block,
you can read and write to

00:02:37.387 --> 00:02:40.001
this array as you normally would.

00:02:40.083 --> 00:02:43.184
In this example, each thread
writes its own index into the

00:02:43.224 --> 00:02:45.604
corresponding element of the array.

00:02:45.704 --> 00:02:49.385
Just keep potential data races
in mind and use sync threads so

00:02:49.425 --> 00:02:54.192
that every thread finishes writing
before any thread reads the data.

00:02:54.326 --> 00:02:57.933
To emphasize that this shared
array is indeed shared across

00:02:57.947 --> 00:03:01.494
all threads in a block, we
have the first thread print

00:03:01.508 --> 00:03:03.851
out every value stored in it.

00:03:03.936 --> 00:03:06.898
The first thread sees the updates
from all other threads, and

00:03:06.918 --> 00:03:10.380
in this case, it prints 0, 1, 2, 3.

00:03:11.842 --> 00:03:15.265
In the exercise for this section,
you'll allocate the block

00:03:15.345 --> 00:03:18.808
histogram in shared memory
instead of in global memory.

