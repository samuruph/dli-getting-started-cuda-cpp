WEBVTT

00:00:01.676 --> 00:00:05.418
At this point, we've learned
how to use asynchrony to overlap

00:00:05.498 --> 00:00:10.679
CPU and GPU tasks, leading
to a big performance boost.

00:00:10.760 --> 00:00:14.841
Our code now copies data from
the GPU to the CPU, launches an

00:00:14.861 --> 00:00:19.333
asynchronous computation with CUB,
and then writes results to disk.

00:00:19.403 --> 00:00:21.950
However, there's still
room for improvement.

00:00:22.004 --> 00:00:25.546
The GPU compute doesn't
actually have to wait for

00:00:25.566 --> 00:00:27.298
the copy to finish.

00:00:27.386 --> 00:00:31.569
We can apply the same overlapping
strategy to the copy and compute

00:00:31.609 --> 00:00:36.690
steps, but this will require
the copy to be asynchronous.

00:00:36.772 --> 00:00:40.875
Fortunately, Thrust itself
doesn't have any special mechanism

00:00:40.935 --> 00:00:42.886
to transfer data.

00:00:42.996 --> 00:00:46.559
Under the hood, it relies
on the CUDA runtime function

00:00:46.639 --> 00:00:50.865
called CUDA mem copy async.

00:00:51.004 --> 00:00:54.491
This function can copy data
between host and device memory

00:00:54.507 --> 00:00:56.118
without blocking.

00:00:56.188 --> 00:00:59.791
Let's take a closer look at
how we can use it to overlap

00:00:59.851 --> 00:01:02.977
our copy and compute steps.

00:01:03.154 --> 00:01:07.197
Let's begin with the CUDA mem
copy async interface, which

00:01:07.257 --> 00:01:09.776
essentially powers ThrustCopy.

00:01:09.859 --> 00:01:12.581
It handles asynchronous data
transfers between host and

00:01:12.601 --> 00:01:16.884
device memory, but there are a few
important pitfalls to keep in mind.

00:01:16.944 --> 00:01:20.837
First, CUDA mem copy
async isn't type-safe.

00:01:20.905 --> 00:01:24.926
It won't verify that the source
and destination types match.

00:01:25.009 --> 00:01:28.912
Second, it operates in bytes
rather than elements, so you

00:01:28.952 --> 00:01:32.375
need to multiply the number
of elements by the size of

00:01:32.455 --> 00:01:35.154
each element to avoid mistakes.

00:01:35.257 --> 00:01:38.500
Third, you must explicitly
indicate whether to transfer

00:01:38.520 --> 00:01:43.384
it host to host, device to
host, device to device, using

00:01:43.444 --> 00:01:47.467
the CUDA memcopy kind parameter.

00:01:47.628 --> 00:01:51.862
Finally, CUDA memcopy async
returns an error code, which

00:01:51.922 --> 00:01:54.900
you should always check.

00:01:55.103 --> 00:02:00.305
Would simply swapping thrust copy
for CUDA memcopy async be enough

00:02:00.345 --> 00:02:02.689
to overlap compute with copy?

00:02:02.745 --> 00:02:03.756
Not really.

00:02:03.806 --> 00:02:08.460
By default, all GPU operations
are ordered on the GPU end.

00:02:08.562 --> 00:02:13.983
Even though CUDA memcopy async is
non-blocking on the CPU side, the

00:02:14.023 --> 00:02:18.352
GPU still executes it in order with
everything else, just like those

00:02:18.364 --> 00:02:21.013
earlier asynchronous compute steps.

00:02:21.105 --> 00:02:25.986
This means that GPU will wait for
copy to finish before proceeding

00:02:26.046 --> 00:02:28.502
to the next compute step.

00:02:28.627 --> 00:02:33.464
This ordering of operations on the
GPU side is called a CUDA stream.

00:02:33.548 --> 00:02:37.049
When you don't specify a CUDA
stream, some default one is

00:02:37.069 --> 00:02:38.796
NVIDIA was first used.

00:02:38.908 --> 00:02:42.010
And the great news is that
we can have as many CUDA

00:02:42.030 --> 00:02:43.678
streams as we need.

00:02:43.751 --> 00:02:47.152
When operations run in different
streams, they are no longer

00:02:47.173 --> 00:02:51.513
ordered with respect to each
other and can execute concurrently.

00:02:51.595 --> 00:02:53.272
That's exactly what we want.

00:02:53.316 --> 00:02:56.678
If we send an asynchronous
copy to one stream and our

00:02:56.718 --> 00:03:01.010
compute work to another, then
those two operations can overlap.

00:03:01.060 --> 00:03:03.827
Let's see how that
works in practice.

00:03:03.962 --> 00:03:08.331
CUDA stream T is simply a
type, just like an integer.

00:03:08.410 --> 00:03:11.172
That means we can declare
our streams the same way we

00:03:11.212 --> 00:03:14.539
declare any other variable in C++.

00:03:14.635 --> 00:03:18.534
In this example, we define two
streams, one for memory transfers

00:03:18.559 --> 00:03:20.680
and another for computations.

00:03:20.761 --> 00:03:24.317
However, just creating a stream
variable isn't enough.

00:03:24.364 --> 00:03:27.346
We also need to explicitly
call CUDA string create

00:03:27.426 --> 00:03:29.081
to initialize it.

00:03:29.179 --> 00:03:31.921
Later, if we want to wait
for the task in a specific

00:03:31.962 --> 00:03:35.384
stream to finish, we can use
CUDA Stream Synchronize, which

00:03:35.465 --> 00:03:40.700
blocks only until the particular
stream's operations are complete.

00:03:40.869 --> 00:03:43.812
This is preferable to CUDA
Device Synchronize, which

00:03:43.892 --> 00:03:49.116
causes CPU to wait until all
GPU operations have completed.

00:03:49.176 --> 00:03:52.879
By synchronizing at the stream
level, we let other streams

00:03:52.939 --> 00:03:55.454
continue to work In parallel,

00:03:55.560 --> 00:03:59.183
Finally, once we're done with
a stream, we should call CUDA

00:03:59.203 --> 00:04:03.673
stream destroy to release
associated resources.

00:04:03.826 --> 00:04:08.557
You'll find CUDA stream T in almost
every accelerated CUDA library.

00:04:08.629 --> 00:04:12.632
For instance, both CUDA mem
copies async and CUB device

00:04:12.672 --> 00:04:17.275
transform have a CUDA stream
T parameter, though we haven't

00:04:17.315 --> 00:04:20.277
had to specify it yet because
it defaults to zero, which Which

00:04:20.337 --> 00:04:23.398
means use the default CUDA stream.

00:04:23.496 --> 00:04:27.817
The reason CUDA Streams is so
widespread is that developers

00:04:27.837 --> 00:04:32.022
often want to overlap these
library calls with other tasks.

00:04:32.098 --> 00:04:35.619
For example, you might run
device-side computations in

00:04:35.639 --> 00:04:40.020
one stream while simultaneously
handling host-side, IO and

00:04:40.080 --> 00:04:43.123
memory transfers in another.

00:04:43.241 --> 00:04:46.478
So let's start by creating
CUDA Streams for compute

00:04:46.502 --> 00:04:48.828
and copy operations.

00:04:48.942 --> 00:04:51.442
After setting up a
separate copy stream,

00:04:51.482 --> 00:04:55.205
We can pass it as the last
argument to CUDA mem copy

00:04:55.285 --> 00:05:01.060
async, so that the data transfer
runs in its own stream.

00:05:01.231 --> 00:05:04.774
Next, we can specify compute
streams as the last parameter

00:05:04.854 --> 00:05:07.226
when calling CUB device transform.

00:05:07.276 --> 00:05:10.679
By doing so, we ensure the
computation runs in a different

00:05:10.719 --> 00:05:13.297
stream than the copy stream.

00:05:13.410 --> 00:05:16.511
Because the copy happens
asynchronously, we have to

00:05:16.551 --> 00:05:20.281
make sure it has actually finished
before reading the host data.

00:05:20.333 --> 00:05:24.655
To do this, we call CUDA stream
synchronize on the copy stream,

00:05:24.695 --> 00:05:30.517
which blocks until operations
in the copy stream are complete.

00:05:31.582 --> 00:05:35.123
After we're done writing the
results to disk, we synchronize

00:05:35.163 --> 00:05:38.522
with the compute stream to
confirm that the GPU computations

00:05:38.544 --> 00:05:40.668
have also finished.

00:05:40.765 --> 00:05:43.687
In the end, our approach looks
like the diagram.

00:05:43.725 --> 00:05:47.346
We perform two asynchronous
operations, one for copying

00:05:47.387 --> 00:05:50.389
data, one for computing, in
separate streams.

00:05:50.447 --> 00:05:54.068
We also include two synchronization
points to ensure everything

00:05:54.128 --> 00:05:57.667
finishes in the right order.

00:05:57.869 --> 00:06:00.086
And Congratulations!

00:06:00.137 --> 00:06:03.454
We've just introduced
our first data race.

00:06:03.518 --> 00:06:07.165
Let's walk through the first
few iterations to see why.

00:06:07.259 --> 00:06:11.796
We issue a copy from
dprev to the host.

00:06:11.900 --> 00:06:16.103
The first compute iteration
reads from dprev and writes

00:06:16.121 --> 00:06:19.464
into dnext, which is fine.

00:06:19.542 --> 00:06:25.529
But the next iteration reads from
dnext and writes back to dprev.

00:06:25.612 --> 00:06:29.499
Because these operations run in
different streams, the GPU can

00:06:29.513 --> 00:06:32.352
interleave them in unexpected ways.

00:06:32.414 --> 00:06:35.355
This means that while data
is still being copied from

00:06:35.375 --> 00:06:40.256
deprev to the host, it might
get overwritten by the next compute

00:06:40.316 --> 00:06:42.613
iteration in another stream.

00:06:42.697 --> 00:06:48.112
If we want correct visualization
results, we'll have to fix this.

00:06:48.278 --> 00:06:50.699
Like many computer science
problems, we can solve

00:06:50.759 --> 00:06:54.165
this by adding another
layer of indirection.

00:06:54.246 --> 00:06:57.969
We can allocate an extra device
buffer, copy data into it

00:06:58.029 --> 00:07:02.884
synchronously, and then copy
from the staging buffer to the CPU.

00:07:02.972 --> 00:07:05.734
Since no one else is writing
to the staging buffer, there's

00:07:05.794 --> 00:07:07.837
no risk of a data race.

00:07:07.916 --> 00:07:10.418
But doesn't that defeat
the purpose of overlapping

00:07:10.438 --> 00:07:12.463
compute with transfers?

00:07:12.539 --> 00:07:15.995
At the first
glance, it may seem so.

00:07:16.162 --> 00:07:18.984
But if you take a closer look
at the bandwidth across your

00:07:19.024 --> 00:07:23.347
entire system, this approach
makes a lot more sense.

00:07:23.410 --> 00:07:27.892
Copying data within GPU memory
can hit speeds of around 1,000

00:07:27.892 --> 00:07:29.864
gigabytes per second.

00:07:29.932 --> 00:07:33.444
Moving data within CPU memory
is somewhat slower, on the

00:07:33.454 --> 00:07:36.971
order of 100 gigabytes per second.

00:07:37.055 --> 00:07:41.597
However, transfers between the
CPU and GPU often go through the

00:07:41.637 --> 00:07:48.723
PCIe bus, which might provide as
little as 32 gigabytes per second.

00:07:48.819 --> 00:07:53.292
That means that the real bottleneck
is cross-memory transfers.

00:07:53.345 --> 00:07:56.042
between the CPU and GPU.

00:07:56.129 --> 00:07:59.593
The device-to-device copy
is going to be so fast, it's

00:07:59.653 --> 00:08:04.871
practically free compared to
the cost of sending data over PCIe.

00:08:04.980 --> 00:08:08.685
So, it still makes sense to
overlap the slower device-to-host

00:08:08.725 --> 00:08:13.589
copy with ongoing GPU computations.

00:08:13.937 --> 00:08:17.467
That brings us to our exercise
for this section.

00:08:17.560 --> 00:08:21.688
In the exercise, you'll replace
thrust copy with CUDA memcopy

00:08:21.704 --> 00:08:25.657
async to make your device-to-host
transfer asynchronous.

00:08:25.727 --> 00:08:29.591
You'll also place your compute
and copy operations on separate

00:08:29.631 --> 00:08:34.290
streams and synchronize them
exactly as shown in the diagram.

00:08:34.355 --> 00:08:37.177
Finally, once you're comfortable
with your changes, you can

00:08:37.217 --> 00:08:38.036
profile the code in NVIDIA.

00:08:38.058 --> 00:08:39.279
IT'S VERY IMPORTANT TO USE
IT'S VERY IMPORTANT TO USE

00:08:39.319 --> 00:08:40.600
INSIGHT SYSTEMS TO SEE IF
IT INSIGHT SYSTEMS TO SEE

00:08:40.620 --> 00:08:41.521
IF IT BEHAVES AS EXPECTED.

