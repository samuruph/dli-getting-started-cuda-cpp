WEBVTT

00:00:00.959 --> 00:00:05.061
In this section, we'll explore the
concept of asynchrony, which lets

00:00:05.122 --> 00:00:08.959
us overlap computation with I-O.

00:00:09.084 --> 00:00:12.942
Let's review the current state
of our heat equation simulator.

00:00:13.006 --> 00:00:16.688
First, we copy the data from the
GPU to the host, then we write

00:00:16.708 --> 00:00:20.671
it to disk, and finally, we use
thrust tabulate to compute the next

00:00:20.711 --> 00:00:23.500
set of temperatures on the GPU.

00:00:23.599 --> 00:00:27.422
Under the hood, Thrust launches
work on the GPU and waits

00:00:27.462 --> 00:00:28.614
for it to finish.

00:00:28.662 --> 00:00:31.584
Because Thrust Tabulate
doesn't return control until

00:00:31.624 --> 00:00:36.175
the transformation is complete,
the CPU stays idle.

00:00:36.247 --> 00:00:37.648
This is a missed opportunity.

00:00:37.668 --> 00:00:41.915
Writing an efficient heterogeneous
program means making full

00:00:41.931 --> 00:00:44.282
use of all resources.

00:00:44.353 --> 00:00:46.060
This includes the CPU as well.

00:00:46.074 --> 00:00:49.436
So, can we find anything
meaningful for the CPU to

00:00:49.496 --> 00:00:52.288
do while the GPU is busy?

00:00:52.423 --> 00:00:56.384
To answer this question, let's take
a look at the data dependencies.

00:00:56.444 --> 00:00:59.326
The write step reads from
the host copy of the previous

00:00:59.366 --> 00:01:03.681
temperatures, while the compute
step reads from the device copy.

00:01:03.767 --> 00:01:07.489
Because the operations use
separate data, they don't

00:01:07.529 --> 00:01:09.159
depend on each other.

00:01:09.238 --> 00:01:12.780
As a result, the CPU can write
results to disk while this

00:01:12.860 --> 00:01:16.028
GPU computes the next temperatures.

00:01:16.083 --> 00:01:19.465
To achieve this overlap,
we'd ideally split the thrust

00:01:19.525 --> 00:01:23.103
call into separate launching
and waiting steps.

00:01:23.167 --> 00:01:26.443
Unfortunately, Thrust
doesn't support that.

00:01:26.510 --> 00:01:29.235
The good news is that
Thrust doesn't implement

00:01:29.251 --> 00:01:31.266
its algorithms directly.

00:01:31.313 --> 00:01:35.816
Instead, it relies on another
core library called CUB to

00:01:35.876 --> 00:01:38.160
handle work on the GPU.

00:01:38.254 --> 00:01:42.199
Let's take a closer look at
KUB to see if it can help

00:01:42.218 --> 00:01:46.376
us overlap compute and I.O.

00:01:46.581 --> 00:01:52.064
Thrust calls into another layer of
the CUDA core libraries called KUB.

00:01:52.126 --> 00:01:55.949
KUB is asynchronous, so once
you launch an operation, it

00:01:56.009 --> 00:01:59.500
immediately hands control
back to the CPU instead of

00:01:59.512 --> 00:02:01.962
waiting for the GPU to finish.

00:02:02.057 --> 00:02:06.710
After launching a CUB algorithm,
Thrust uses CUDA device synchronize

00:02:06.758 --> 00:02:09.298
to wait for CUB completion.

00:02:09.379 --> 00:02:13.189
This CUDA device synchronize
is part of the CUDA runtime.

00:02:13.240 --> 00:02:17.360
It essentially blocks the CPU
thread until all preceding

00:02:17.400 --> 00:02:20.887
GPU operations have completed.

00:02:21.061 --> 00:02:24.502
CUB offers many of the same
parallel algorithms found

00:02:24.562 --> 00:02:29.175
in Thrust, but it doesn't
follow the standard C++ API.

00:02:29.355 --> 00:02:32.997
It's also CUDA-specific, meaning
it doesn't provide any host-side

00:02:33.037 --> 00:02:36.502
implementations like Thrust does.

00:02:36.699 --> 00:02:40.501
Asynchrony is a key concept
in GPU programming, and this

00:02:40.581 --> 00:02:42.520
example shows why.

00:02:42.602 --> 00:02:45.812
In the first snippet, we
use Thrust tabulate.

00:02:45.864 --> 00:02:50.587
Thrust is synchronous, so our
timer doesn't stop until the GPU

00:02:50.607 --> 00:02:53.420
has completely finished the work.

00:02:53.508 --> 00:02:56.730
In the second snippet, we
use CUB's device transform

00:02:56.770 --> 00:02:59.389
function, which is asynchronous.

00:02:59.465 --> 00:03:03.043
That means it returns control
to the CPU right away, even

00:03:03.068 --> 00:03:05.847
if the GPU is still computing.

00:03:05.950 --> 00:03:09.402
As a result, when we record
the end time, the GPU hasn't

00:03:09.432 --> 00:03:14.343
actually finished yet, so it looks
like CUB takes no time at all.

00:03:14.436 --> 00:03:18.480
Increasing the problem size doesn't
change that timing because the CPU

00:03:18.498 --> 00:03:21.532
never waits for the GPU to finish.

00:03:21.641 --> 00:03:25.859
This illustrates what asynchronous
execution really means.

00:03:25.924 --> 00:03:30.031
The CPU and GPU can
work in parallel.

00:03:30.134 --> 00:03:33.979
That's why timing measurements
can be misleading if you

00:03:34.019 --> 00:03:37.964
don't explicitly synchronize
before checking how long

00:03:38.024 --> 00:03:41.106
the GPU work took.

00:03:42.344 --> 00:03:44.444
So let's do exactly that.

00:03:44.486 --> 00:03:48.269
By adding the CUDA device
synchronize call, we force

00:03:48.329 --> 00:03:51.886
the CPU to wait until the
GPU work is complete.

00:03:51.932 --> 00:03:55.455
Once we add this synchronization,
both Cub and Thrust show equivalent

00:03:55.475 --> 00:03:56.916
performance results.

00:03:56.976 --> 00:03:59.519
The question then becomes,
how do we take advantage of

00:03:59.559 --> 00:04:03.120
this asynchronous behavior?

00:04:04.392 --> 00:04:07.634
Now we can finally split our
thrust call into separate

00:04:07.674 --> 00:04:09.340
launch and wait steps.

00:04:09.415 --> 00:04:12.196
When we launch asynchronous
work with KUB, the CPU is

00:04:12.236 --> 00:04:16.018
freed immediately, allowing
us to start writing data to disk

00:04:16.059 --> 00:04:18.858
while the GPU continues computing.

00:04:18.980 --> 00:04:22.972
We then synchronize with the
GPU only after it finishes.

00:04:23.022 --> 00:04:25.304
Since we would have waited
for the GPU anyway, we're

00:04:25.324 --> 00:04:26.384
not losing any time.

00:04:26.484 --> 00:04:29.426
Instead, the wait step becomes
shorter, leading to a more

00:04:29.486 --> 00:04:31.827
efficient use of the system.

00:04:31.934 --> 00:04:35.096
As a result, we can expect
a significant performance

00:04:35.176 --> 00:04:38.039
boost from this approach.

00:04:38.198 --> 00:04:41.160
In the first exercise for
this section, you'll replace

00:04:41.200 --> 00:04:44.702
thrust tabulate with CUB
device transform to make

00:04:44.742 --> 00:04:47.013
the computation asynchronous.

00:04:47.064 --> 00:04:50.926
You'll also add a CUDA device
synchronized call after writing

00:04:50.946 --> 00:04:55.430
the results to disk following
the scheme shown on the right.

00:04:55.569 --> 00:04:59.031
As we dive deeper into asynchronous
execution, profiling and

00:04:59.051 --> 00:05:00.915
debugging can get tricky.

00:05:00.976 --> 00:05:04.378
It's not always clear when
tasks overlap, or if the CPU

00:05:04.398 --> 00:05:06.759
and GPU are being fully utilized.

00:05:06.819 --> 00:05:11.237
This is where the NVIDIA profiling
tool, Ansight Systems, steps in.

00:05:11.302 --> 00:05:13.943
Ansight Systems gives
you a system-wide view

00:05:14.003 --> 00:05:17.947
of both GPU and CPU activities,
visually representing

00:05:17.965 --> 00:05:19.898
asynchronous operations.

00:05:19.967 --> 00:05:23.809
By showing you exactly when and
where each task runs, it highlights

00:05:23.869 --> 00:05:28.778
potential bottlenecks and uncovers
new opportunities for optimization.

00:05:28.877 --> 00:05:32.604
In the Insight Profiling exercise,
you'll generate a profile

00:05:32.620 --> 00:05:36.892
and look around the timeline
to identify when GPU computation

00:05:36.904 --> 00:05:41.408
is launched, when CPU writes
data on disk, when CPU waits

00:05:41.488 --> 00:05:47.963
for GPU, and finally, when the data
is transferred between CPU and GPU.

00:05:48.094 --> 00:05:51.017
In the final exercise,
you'll work with the NVIDIA

00:05:51.077 --> 00:05:53.903
Tools extension, NVTX.

00:05:53.988 --> 00:05:57.292
When you're working on a complex
application, interpreting an

00:05:57.310 --> 00:05:59.665
insight report can be overwhelming.

00:05:59.711 --> 00:06:04.073
That's where NVTX and NVIDIA
Tools extension come in.

00:06:04.153 --> 00:06:07.661
NVTX allows you to insert
custom markers and ranges

00:06:07.695 --> 00:06:11.317
directly into your code, so that
when you view the insight report,

00:06:11.377 --> 00:06:13.708
your labels appear in the timeline.

00:06:13.778 --> 00:06:19.349
In the code on the side, we
use NVTX3 scoped range to

00:06:19.361 --> 00:06:21.534
label each right step.

00:06:21.616 --> 00:06:24.793
The range starts at the beginning
of the loop iteration and

00:06:24.818 --> 00:06:27.779
ends automatically when it
goes out of scope.

00:06:27.859 --> 00:06:31.276
This helps you instantly
map parts of the timeline

00:06:31.321 --> 00:06:34.103
to the code you write.

00:06:34.242 --> 00:06:38.544
In the optional NBTX exercise,
you're asked to annotate the

00:06:38.604 --> 00:06:43.224
copy, compute, write, and
wait steps using NBTX ranges.

00:06:43.286 --> 00:06:46.728
Then profile the simulator
with Insight Systems and try

00:06:46.808 --> 00:06:48.449
locating the specific ranges.

