WEBVTT

00:00:00.686 --> 00:00:04.268
Our simulator is coming together
nicely, but constantly writing

00:00:04.348 --> 00:00:08.637
out all the data to disk for
visualization can be overkill.

00:00:08.709 --> 00:00:11.455
Instead, we can track
something simpler, like

00:00:11.491 --> 00:00:13.669
per row total temperature.

00:00:13.752 --> 00:00:16.753
This way we can easily see how
much heat accumulates at the

00:00:16.793 --> 00:00:21.245
boundaries over time without having
to dump every cell's temperature.

00:00:21.335 --> 00:00:24.934
To visualize this idea, imagine
that we take each row of our

00:00:24.956 --> 00:00:28.846
2D grid, add up the temperatures
of every cell in the row.

00:00:28.894 --> 00:00:31.757
And store the sum in a 1D vector.

00:00:31.835 --> 00:00:35.257
In this example, the first
row has 3 cells at 5 degrees

00:00:35.337 --> 00:00:39.452
each, so we store 15 as the
first element of the vector.

00:00:39.538 --> 00:00:43.540
The second row has 3 cells at
3 degrees each, so we store

00:00:43.540 --> 00:00:46.341
9 in the second element, and so on.

00:00:46.401 --> 00:00:50.429
The question is, how do we
implement this row-wise aggregation

00:00:50.463 --> 00:00:53.416
in a clean and efficient way?

00:00:53.564 --> 00:00:57.861
A straightforward approach is to
reach for thrust tabulate again.

00:00:57.912 --> 00:01:02.722
But this time we tabulate over row
indices instead of cell indices.

00:01:02.817 --> 00:01:05.520
Inside the lambda, we loop
through each column in the

00:01:05.560 --> 00:01:09.201
row, accumulate the total
temperature, and return that sum.

00:01:09.263 --> 00:01:10.881
But is this the best way to do it?

00:01:10.905 --> 00:01:14.648
Let's think about the potential
performance implications.

00:01:14.769 --> 00:01:17.451
When we write our code this
way, it's important to keep

00:01:17.491 --> 00:01:20.494
in mind that the GPU doesn't
automatically We parallelize

00:01:20.555 --> 00:01:22.524
the loop inside our lambda.

00:01:22.588 --> 00:01:26.773
Thrust only parallelizes at the
level of rows because we're calling

00:01:26.789 --> 00:01:30.200
thrust tabulate on row indices.

00:01:30.311 --> 00:01:34.190
If you have millions of columns,
but only a handful of rows,

00:01:34.232 --> 00:01:39.469
you end up with just a few threads
doing massive work on each one.

00:01:39.594 --> 00:01:42.315
That means you're leaving
most of the GPU's parallel

00:01:42.355 --> 00:01:47.990
potential on the table, effectively
using less than 1% of its capacity.

00:01:48.077 --> 00:01:50.133
So how do we fix this?

00:01:50.222 --> 00:01:53.169
We need to somehow distribute
the work across many threads.

00:01:53.249 --> 00:01:56.737
In other words, we want
each column in a row to

00:01:56.777 --> 00:02:00.040
be handled in parallel too.

00:02:00.270 --> 00:02:03.739
We'll begin by naming the
operation we're trying to perform.

00:02:03.811 --> 00:02:07.512
If we ignore the fact that we have
a row dimension, it basically looks

00:02:07.552 --> 00:02:12.197
like a standard reduction, except
we're grouping values by row.

00:02:12.254 --> 00:02:15.868
That's exactly where a thrust
reduced by key comes in.

00:02:15.935 --> 00:02:19.076
Unlike a plain reduce,
reduced by key takes two

00:02:19.156 --> 00:02:21.994
input ranges, keys and values.

00:02:22.096 --> 00:02:25.177
It then groups together values
that show the same key and

00:02:25.237 --> 00:02:28.085
reduces each group independently.

00:02:28.177 --> 00:02:32.332
So in our case, the row index
naturally becomes our key.

00:02:32.418 --> 00:02:35.779
All the cells in a given row
share the same key, and we

00:02:35.819 --> 00:02:38.337
can sum those cell values together.

00:02:38.419 --> 00:02:41.524
In this way, we spread the
work across many threads

00:02:41.560 --> 00:02:46.379
while still ending up with
one final sum per row.

00:02:46.581 --> 00:02:50.002
To use reduceByKey, we need
each cell in the grid to

00:02:50.082 --> 00:02:52.191
know its row key.

00:02:52.295 --> 00:02:56.646
We'll create a rows ID array
where each entry corresponds

00:02:56.658 --> 00:03:02.103
to the row index for the cell
at position I. In the code snippet,

00:03:02.121 --> 00:03:07.100
we do that by calling thrust
tabulate on our row IDs vector,

00:03:07.144 --> 00:03:11.254
passing a lambda that returns
I divided by the grid width.

00:03:11.346 --> 00:03:15.429
That way, every cell in row
zero gets a key of zero, every

00:03:15.509 --> 00:03:20.589
cell in row one gets a key
of one, and so on.

00:03:20.789 --> 00:03:25.653
Once we have our row IDs array,
we can call thrust reduced by key.

00:03:25.713 --> 00:03:30.017
We pass row IDs begin and row
IDs end as input keys, where

00:03:30.077 --> 00:03:32.943
each entry corresponds to the row.

00:03:33.019 --> 00:03:36.171
We pass temp begin for
the input values.

00:03:36.222 --> 00:03:39.090
These are the temperatures
in our 2D grid flattened

00:03:39.104 --> 00:03:40.743
into one dimension.

00:03:40.826 --> 00:03:45.205
We pass thrust make discard
iterator as the output keys,

00:03:45.249 --> 00:03:47.200
which we'll discuss in a moment.

00:03:47.271 --> 00:03:50.080
And finally, we pass SUMS BEGIN

00:03:50.137 --> 00:03:54.648
As the output values where the
final per row sums will be written.

00:03:54.721 --> 00:03:58.600
This way all cells with the
same row key get added together

00:03:58.644 --> 00:04:03.417
and we end up with one sum
per row in the sums vector.

00:04:03.568 --> 00:04:06.921
By default, reduced by key
writes both keys and values

00:04:06.931 --> 00:04:08.578
into output iterators.

00:04:08.632 --> 00:04:11.454
But in our case, we already
know each row corresponds

00:04:11.514 --> 00:04:13.333
to a unique key in order.

00:04:13.376 --> 00:04:16.481
So writing out those row IDs
would be redundant.

00:04:16.533 --> 00:04:19.942
To save memory bandwidth,
we use thrust make discard

00:04:19.956 --> 00:04:22.281
iterator for the key output.

00:04:22.358 --> 00:04:25.441
A discard iterator simply
ignores any values written

00:04:25.481 --> 00:04:29.170
to it, so we skip storing
the integer sequence and keep

00:04:29.184 --> 00:04:32.296
only the per row sums in sums.

00:04:32.407 --> 00:04:36.611
These changes translate into
roughly 100-fold speedup compared

00:04:36.651 --> 00:04:40.314
to our initial approach, which
relied on tabulate with a

00:04:40.414 --> 00:04:42.386
loop in the lambda.

00:04:42.472 --> 00:04:45.173
It's a striking reminder
that GPUs don't magically

00:04:45.233 --> 00:04:46.795
parallelize your code.

00:04:46.834 --> 00:04:50.515
You need to design your data
processing so all those GPU

00:04:50.535 --> 00:04:52.745
threads can share the workload.

00:04:52.816 --> 00:04:55.434
Whenever you see a loop
inside a device Lambda,

00:04:55.456 --> 00:05:00.106
ask yourself if there's
a more parallel way to do it.

00:05:00.278 --> 00:05:03.259
We've eliminated most sources
of inefficiency, but there's

00:05:03.279 --> 00:05:04.478
still one more issue.

00:05:04.500 --> 00:05:08.261
Right now, our algorithm makes
two passes through memory,

00:05:08.321 --> 00:05:11.777
one for the temperature values
and another for the row indices.

00:05:11.837 --> 00:05:16.188
We don't actually need to store
those row IDs in memory at all.

00:05:16.240 --> 00:05:19.462
Think about how we generated
the row indices by transforming

00:05:19.542 --> 00:05:25.896
cell indices, 0, 1, 2, and
so on into row IDs, i over width.

00:05:25.987 --> 00:05:29.739
Instead of storing that result
in the row IDs array, we can

00:05:29.749 --> 00:05:31.394
use fancy iterators.

00:05:31.471 --> 00:05:34.553
A counting iterator for the
cells, plus a transform iterator

00:05:34.593 --> 00:05:38.746
that converts each cell index
into a row index on the fly.

00:05:38.854 --> 00:05:42.246
That way, we never write the
row IDs out to memory.

00:05:42.296 --> 00:05:46.739
Your first exercise is to replace
the explicit row IDs vector with

00:05:46.779 --> 00:05:52.695
these iterators so we have only one
real set of data reads and writes.

00:05:52.904 --> 00:05:55.385
The second exercise is
optional and includes

00:05:55.425 --> 00:05:58.057
the transform output iterator.

00:05:58.127 --> 00:06:01.217
So far, we've been transforming
values as we read them

00:06:01.229 --> 00:06:02.780
from the iterator.

00:06:02.873 --> 00:06:06.814
But you can also transform
values as you write them by

00:06:06.914 --> 00:06:10.459
adding another level of indirection
on the output side.

00:06:10.555 --> 00:06:13.516
In the example, we return
a tiny wrapper object from

00:06:13.556 --> 00:06:16.223
the iterator's subscript operator.

00:06:16.337 --> 00:06:19.968
When we assign a value to
that wrapper, its assignment

00:06:19.978 --> 00:06:22.933
operator applies a transformation.

00:06:23.039 --> 00:06:26.870
In our case, it divides the
incoming value by 2 before

00:06:26.880 --> 00:06:29.797
storing it in the underlying array.

00:06:29.881 --> 00:06:32.556
This leads to an
interesting behavior.

00:06:32.643 --> 00:06:38.109
So if you assign it at index
0 a value of 10, 10 is divided

00:06:38.169 --> 00:06:43.314
by 2 and the array element
actually ends up storing 5.

00:06:43.495 --> 00:06:47.577
In many ways, a transform
output iterator is symmetrical

00:06:47.599 --> 00:06:51.363
to the transform iterator we've
already used, except this time

00:06:51.444 --> 00:06:55.359
the transformation happens when
writing instead of Good reading.

00:06:55.450 --> 00:07:00.197
So, instead of taking raw data and
returning modified values, it takes

00:07:00.277 --> 00:07:06.584
the values you assign and writes a
transformed version back to memory.

00:07:07.847 --> 00:07:12.211
This transform output iterator
idea is really handy in our

00:07:12.311 --> 00:07:14.180
row temperature scenario.

00:07:14.253 --> 00:07:18.777
Right now, each row sum can be
a very large number, which isn't

00:07:18.857 --> 00:07:20.811
always meaningful in practice.

00:07:20.859 --> 00:07:24.878
We're more interested in the
average temperature per row.

00:07:24.943 --> 00:07:28.146
Of course, we could do another
pass to divide those sums

00:07:28.226 --> 00:07:31.649
by the row width, but that
would mean extra reads and

00:07:31.669 --> 00:07:34.594
writes, which we'd prefer to avoid.

00:07:34.699 --> 00:07:39.441
Instead we can use a transform
output iterator to convert

00:07:39.501 --> 00:07:43.897
the totals into means on the
fly as we write them out.

00:07:43.983 --> 00:07:49.157
So in this optional exercise,
replace the final output iterator

00:07:49.186 --> 00:07:53.147
in the reduce by key call
with a transform output iterator

00:07:53.187 --> 00:07:56.709
that divides each row sum
by the grid width.

