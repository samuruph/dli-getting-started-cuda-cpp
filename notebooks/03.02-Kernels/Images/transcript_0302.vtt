WEBVTT

00:00:00.825 --> 00:00:04.810
Up to this point, we've worked with
CUDA libraries to manage our code.

00:00:04.868 --> 00:00:08.091
The CPU typically calls host
functions while the GPU calls

00:00:08.151 --> 00:00:09.522
device functions.

00:00:09.592 --> 00:00:14.750
Invoking GPU work from the CPU
requires a special mechanism.

00:00:14.837 --> 00:00:19.290
So far, parallel algorithms
handled this magic for us,

00:00:19.340 --> 00:00:22.919
seamlessly calling GPU code behind

00:00:23.064 --> 00:00:26.006
Now that we want to implement
our own algorithms without

00:00:26.066 --> 00:00:29.389
relying on those libraries,
we need to understand how

00:00:29.429 --> 00:00:32.482
to launch GPU code ourselves.

00:00:32.591 --> 00:00:35.263
But before diving in,
let's quickly recap the

00:00:35.273 --> 00:00:39.083
execution space specifiers
we've encountered so far.

00:00:39.155 --> 00:00:43.539
Host functions are compiled into
CPU instructions, called from

00:00:43.579 --> 00:00:47.436
the host, and executed on the host.

00:00:47.601 --> 00:00:48.703
Similarly,

00:00:48.755 --> 00:00:52.678
Device functions are compiled into
GPU instructions, called from the

00:00:52.718 --> 00:00:55.922
device, and executed on the device.

00:00:56.021 --> 00:00:58.865
There's another annotation
we haven't discussed yet.

00:00:58.924 --> 00:01:02.687
When you mark a function with
global, it's compiled into GPU

00:01:02.727 --> 00:01:08.051
instructions and executed on the
GPU, but it's called from the CPU.

00:01:08.111 --> 00:01:11.540
These functions are
known as CUDA kernels.

00:01:11.610 --> 00:01:15.393
Calling a CUDA kernel from
the CPU looks unusual because

00:01:15.433 --> 00:01:19.280
of the triple angled brackets
and some numbers inside.

00:01:19.336 --> 00:01:22.907
We'll talk about the numbers and
what they mean in a few minutes.

00:01:22.979 --> 00:01:27.258
Another important detail is that
the kernel calls are asynchronous,

00:01:27.303 --> 00:01:30.026
much like calling a kub algorithm.

00:01:30.085 --> 00:01:34.858
The CPU doesn't wait for the
kernel to finish on the GPU.

00:01:35.009 --> 00:01:38.255
Before we start writing our
own CUDA kernels, let's recap

00:01:38.271 --> 00:01:40.897
how the simulator currently works.

00:01:40.987 --> 00:01:45.008
We use an MD-SPAN for a 2D
view of the temperature grid.

00:01:45.071 --> 00:01:48.275
Then we transform cell indices
into new temperatures with

00:01:48.293 --> 00:01:50.162
CUB device transform.

00:01:50.215 --> 00:01:53.177
Under the hood, CUB implements
device transform using CUDA

00:01:53.238 --> 00:01:56.772
kernels, which is why they
run asynchronously.

00:01:56.841 --> 00:02:00.324
To see how CUDA kernels actually
work, it's best to start with

00:02:00.404 --> 00:02:02.599
code we're already familiar with.

00:02:02.666 --> 00:02:05.348
So let's try rewriting
our simulator to launch

00:02:05.428 --> 00:02:09.851
kernels directly, rather
than relying on CUB.

00:02:09.969 --> 00:02:13.496
In this code snippet, we have a
simple CUDA kernel that iterates

00:02:13.510 --> 00:02:17.909
over all cells and transforms their
indices into new temperatures.

00:02:17.972 --> 00:02:21.793
Notice the triple chevron syntax
when launching it and that the

00:02:21.853 --> 00:02:24.323
fourth parameter is a CUDA stream.

00:02:24.373 --> 00:02:28.467
By passing a stream here, we can
run the computation asynchronously

00:02:28.495 --> 00:02:30.795
on the specified stream.

00:02:30.916 --> 00:02:33.776
But as you already know,
the GPU doesn't magically

00:02:33.836 --> 00:02:35.872
parallelize code on its own.

00:02:35.919 --> 00:02:40.495
This means that a single GPU thread
is computing each cell in sequence.

00:02:40.542 --> 00:02:43.684
Overall, this makes our
kernel roughly 10,000

00:02:43.684 --> 00:02:46.092
times slower than CUB.

00:02:46.185 --> 00:02:50.167
Fortunately, kernels are inherently
parallel, so we can increase

00:02:50.207 --> 00:02:51.536
the number of threads.

00:02:51.588 --> 00:02:54.436
For instance, to launch
two threads, we pass two

00:02:54.450 --> 00:02:57.681
as the second argument in
the triple chevron syntax.

00:02:57.752 --> 00:03:00.813
You can see on the right that
doing so nearly doubles the

00:03:00.873 --> 00:03:05.692
kernel speed, but simply launching
more threads isn't enough.

00:03:05.757 --> 00:03:09.610
If they all process the same
cells, there's no actual speedup.

00:03:09.660 --> 00:03:13.227
To fix this, we need to modify
the kernel so each thread

00:03:13.243 --> 00:03:18.058
can distinguish itself and
handle a different subset of cells.

00:03:18.207 --> 00:03:21.245
That's where ThreadIdx.x comes in.

00:03:21.290 --> 00:03:25.969
ThreadIdx.x is a built-in variable
available in CUDA kernels.

00:03:26.033 --> 00:03:28.703
It holds the index of
the current thread.

00:03:28.756 --> 00:03:33.645
So if we launch two threads,
ThreadIdx.x will be equal to one in

00:03:33.660 --> 00:03:36.768
one thread and zero in the other.

00:03:36.929 --> 00:03:39.090
With this approach,
each thread processes

00:03:39.170 --> 00:03:41.351
a different subset of cells.

00:03:41.411 --> 00:03:45.333
In the kernel, we first read
thread idx.x to identify the

00:03:45.413 --> 00:03:47.306
current thread's index.

00:03:47.374 --> 00:03:50.596
Then each thread starts by
processing the cell whose

00:03:50.676 --> 00:03:53.631
index matches its own thread index.

00:03:53.717 --> 00:03:57.659
That means that the first thread
works on cell zero and the

00:03:57.739 --> 00:04:00.097
second thread works on cell one.

00:04:00.191 --> 00:04:04.072
After finishing one cell, we move
ahead by adding the total number

00:04:04.112 --> 00:04:06.492
of threads to the cell index.

00:04:06.552 --> 00:04:10.073
This way, the first thread
takes cell two next, while

00:04:10.113 --> 00:04:13.992
the second thread moves on
to cell three, and so on.

00:04:14.094 --> 00:04:18.277
Notice how this scheme allows us
to compute all cells in parallel,

00:04:18.315 --> 00:04:22.012
such that one cell is never
processed by multiple threads.

00:04:22.076 --> 00:04:25.677
Another observation is that there's
nothing specific in our code that

00:04:25.697 --> 00:04:27.954
would limit it to only two threads.

00:04:28.007 --> 00:04:30.629
So let's try increasing the
number of threads working

00:04:30.709 --> 00:04:32.983
on this CUDA kernel.

00:04:33.130 --> 00:04:36.953
Here we've increased the number
of threads to 256, and this

00:04:37.013 --> 00:04:38.918
scheme still works great.

00:04:38.974 --> 00:04:41.896
Our performance has improved
significantly, but we're still

00:04:41.956 --> 00:04:44.113
nowhere near CUB's speed.

00:04:44.177 --> 00:04:48.415
Let's see if adding even more
threads can breach this gap.

00:04:48.560 --> 00:04:52.931
Ah, now we've launched 2048
threads, and suddenly everything

00:04:52.943 --> 00:04:57.628
fails with a cryptic invalid
configuration argument error.

00:04:57.711 --> 00:05:00.408
As usual, this means there's
a gap in our understanding.

00:05:00.453 --> 00:05:02.694
It turns out that there's a
limit on the number of threads

00:05:02.754 --> 00:05:03.859
in a single block.

00:05:03.895 --> 00:05:07.561
You cannot launch more
than 1,024 threads.

00:05:07.617 --> 00:05:11.539
So what exactly is a thread
block and why does it matter

00:05:11.619 --> 00:05:13.878
for our kernel launch?

00:05:14.001 --> 00:05:17.223
In CUDA, threads are grouped into
blocks and the entire collection

00:05:17.243 --> 00:05:19.922
of these blocks is called a grid.

00:05:20.005 --> 00:05:24.494
As we've seen already, each block
can contain up to 1,024 threads.

00:05:24.693 --> 00:05:29.207
And every block in a single
kernel launch has the same size.

00:05:29.315 --> 00:05:33.515
The threadidx.x variable we've
seen so far stores the threads

00:05:33.537 --> 00:05:36.530
local index within its block.

00:05:36.598 --> 00:05:40.260
For example, if we have three
blocks, each block will have

00:05:40.300 --> 00:05:44.181
a thread with threadidx.x
set to zero and another thread with

00:05:44.221 --> 00:05:47.408
threadidx set to one and so on.

00:05:47.463 --> 00:05:51.440
That means we end up with
three threads labeled zero,

00:05:51.464 --> 00:05:54.714
three labeled one and so forth.

00:05:54.878 --> 00:05:59.464
Just as we've used threadidx.x
to distinguish threads within

00:05:59.504 --> 00:06:04.122
a block, we also need a way
to identify which block we're in.

00:06:04.189 --> 00:06:07.747
To do that, CUDA provides
additional built-in variables.

00:06:07.874 --> 00:06:12.258
Blockdim.exe tells us how many
threads there are in each block.

00:06:12.338 --> 00:06:17.069
In our example here, every
thread sees two for block dim.x.

00:06:17.119 --> 00:06:21.329
Meanwhile, grid dim.x gives the
number of blocks in the grid.

00:06:21.381 --> 00:06:25.102
So if grid dim.x is three,
that means that there are

00:06:25.182 --> 00:06:27.252
three blocks total.

00:06:27.362 --> 00:06:32.924
By multiplying block dim.x by
grid dim.x, we can get the total

00:06:32.984 --> 00:06:35.653
number of threads in the grid.

00:06:35.745 --> 00:06:40.365
For this example, two times three
gives us six threads in total.

00:06:40.501 --> 00:06:42.882
Besides knowing the total
number of threads, we also

00:06:42.942 --> 00:06:46.400
need each thread's unique
index across the entire grid.

00:06:46.444 --> 00:06:48.884
For that, we use another
built-in variable called

00:06:48.906 --> 00:06:54.070
blockidx.x, which holds
the block's index within the grid.

00:06:54.149 --> 00:06:57.891
All threads in block 0
see blockidx.x equals

00:06:57.891 --> 00:07:01.546
0, while all threads in
block 1 see 1, and so on.

00:07:01.613 --> 00:07:06.296
To calculate a thread's global
index, we multiply blockidx.x

00:07:06.397 --> 00:07:10.398
by blockdim.x to find out
how many threads there are

00:07:10.478 --> 00:07:12.982
before the current block.

00:07:13.098 --> 00:07:17.399
Then we add threadidx.x,
which is the thread's local

00:07:17.459 --> 00:07:19.627
index, in its block.

00:07:19.699 --> 00:07:25.320
For example, the second thread of
the first block multiplies 2 by

00:07:25.320 --> 00:07:31.169
0, then adds 1 for its local index,
giving it a global index of 1.

00:07:31.301 --> 00:07:34.697
The same computation
results in index 5.

00:07:34.804 --> 00:07:38.993
for the last thread
of the last block.

00:07:39.574 --> 00:07:43.436
So now we know how to find
each thread's unique index

00:07:43.476 --> 00:07:47.487
within the grid, and we also
know the total number of threads.

00:07:47.538 --> 00:07:48.798
But a few questions remain.

00:07:48.858 --> 00:07:52.238
For example, what block
size should we use?

00:07:52.280 --> 00:07:55.103
Unfortunately, there's no
one-size-fits-all answer.

00:07:55.161 --> 00:07:58.128
This often comes
down to optimization.

00:07:58.203 --> 00:08:01.584
One helpful guideline is to
pick block sizes in multiples

00:08:01.644 --> 00:08:06.183
of 32 because of how the GPU
hardware groups threads.

00:08:06.263 --> 00:08:10.645
A common default is 256 threads
per block, which we'll use

00:08:10.685 --> 00:08:12.971
throughout this course.

00:08:13.126 --> 00:08:16.183
The second question is how
many blocks we should launch.

00:08:16.248 --> 00:08:19.759
Because we typically want to
maximize parallelism, the grid size

00:08:19.769 --> 00:08:21.782
often depends on the problem size.

00:08:21.830 --> 00:08:25.572
For example, if we want each thread
to handle exactly one element, a

00:08:25.612 --> 00:08:28.079
simple integer division won't work.

00:08:28.153 --> 00:08:31.394
Imagine that we have six cells
to process and choose a block

00:08:31.434 --> 00:08:33.511
size of four threads.

00:08:33.595 --> 00:08:38.014
When we divide six by four,
we get one, which means only

00:08:38.038 --> 00:08:42.325
one block is launched and some
threads end up with extra cells.

00:08:42.401 --> 00:08:46.004
Instead, we can use CUDA's
ceiling divide to perform

00:08:46.044 --> 00:08:48.955
an integer division that rounds up.

00:08:49.066 --> 00:08:53.509
Using ceiling divide gives us two,
ensuring enough blocks so each

00:08:53.589 --> 00:08:56.278
thread processes exactly one cell.

00:08:56.371 --> 00:09:00.054
To specify that we want two
thread blocks, it's sufficient

00:09:00.094 --> 00:09:04.437
to pass two as the first parameter
of the triple chevron.

00:09:04.562 --> 00:09:07.763
With these changes we're now
launching around 5 million

00:09:07.803 --> 00:09:11.416
threads and finally getting
close to CUBS performance.

00:09:11.464 --> 00:09:14.205
Of course CUBS still has more
advanced optimizations under

00:09:14.245 --> 00:09:17.846
the hood, so fully matching
its speed is unlikely, but

00:09:17.886 --> 00:09:23.001
for such a simple kernel this is
already a significant improvement.

00:09:23.168 --> 00:09:26.509
So far we've modified our
simulator but haven't yet confirmed

00:09:26.529 --> 00:09:28.311
that it still works correctly.

00:09:28.370 --> 00:09:30.970
Because our setup is symmetric,
temperatures at the top and

00:09:30.990 --> 00:09:32.984
the bottom of the grid NVIDIA
and NVID are identical.

00:09:33.046 --> 00:09:37.032
We can do a quick smoke test
by verifying that symmetry

00:09:37.069 --> 00:09:39.143
in the first exercise.

00:09:39.250 --> 00:09:40.353
The idea is simple.

00:09:40.371 --> 00:09:43.853
For a given cell, we find its
mirrored row by subtracting the

00:09:43.893 --> 00:09:46.256
rows index from the grid's height.

00:09:46.335 --> 00:09:49.217
If everything is correct,
temperatures of these symmetric

00:09:49.277 --> 00:09:52.064
cells should be nearly the same.

00:09:52.139 --> 00:09:55.061
In this exercise, you'll turn
this function into a CUDA

00:09:55.101 --> 00:10:00.156
kernel and launch it with
the triple Chevron syntax.

00:10:00.353 --> 00:10:03.928
Now, when you think about it, the
entire row should be symmetrical,

00:10:03.939 --> 00:10:05.630
not just a single cell.

00:10:05.681 --> 00:10:08.866
In the second exercise, you'll
modify the kernel so that each

00:10:08.946 --> 00:10:10.989
thread handles exactly one column.

