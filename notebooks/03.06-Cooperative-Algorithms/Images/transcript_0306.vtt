WEBVTT

00:00:01.215 --> 00:00:04.332
Authoring a CUDA kernel doesn't
mean you have to reinvent

00:00:04.356 --> 00:00:06.765
every algorithm from scratch.

00:00:06.857 --> 00:00:09.979
CUDA offers a range of cooperative
libraries that speed up

00:00:10.019 --> 00:00:12.476
your kernels and shorten
development time.

00:00:12.520 --> 00:00:16.129
For example, KUB provides
general-purpose cooperative

00:00:16.141 --> 00:00:20.699
algorithms for reductions,
prefix sums, and sorting.

00:00:20.783 --> 00:00:25.085
Meanwhile, kubelastdx handles
cooperative linear algebra

00:00:25.105 --> 00:00:29.936
functions, and kubfftdx
offers Cooperative.

00:00:29.980 --> 00:00:32.843
Fourier transforms.

00:00:33.022 --> 00:00:36.838
But what exactly does cooperative
mean in this context?

00:00:36.925 --> 00:00:39.187
To explain what cooperative
means, let's review the

00:00:39.227 --> 00:00:42.272
types of algorithms we've
encountered so far.

00:00:42.369 --> 00:00:46.457
We begin with STD sort, which
is a serial algorithm.

00:00:46.512 --> 00:00:50.698
A serial algorithm is both called
and executed by a single thread.

00:00:50.755 --> 00:00:54.463
So if two different threads
each call a serial algorithm,

00:00:54.497 --> 00:00:59.483
the inputs from one thread cannot
affect the outputs of the other.

00:00:59.639 --> 00:01:02.240
Cooperative algorithms, on
the other hand, are invoked

00:01:02.300 --> 00:01:07.169
by multiple threads and are
executed cooperatively among them.

00:01:07.222 --> 00:01:10.893
You can imagine the input of these
cooperative algorithms as one large

00:01:10.903 --> 00:01:15.530
virtual array and just happens to
be split among different threads.

00:01:15.584 --> 00:01:17.245
The same goes for the output.

00:01:17.285 --> 00:01:21.612
In the example shown, a cooperative
sort takes elements one, three

00:01:21.646 --> 00:01:25.001
from the first thread and
two, four from the second.

00:01:25.048 --> 00:01:29.544
After sorting cooperatively, The
first thread's output becomes 1, 2.

00:01:29.719 --> 00:01:33.954
even though two originally
came from the second thread.

00:01:34.040 --> 00:01:36.881
We've also used parallel
algorithms, which are called

00:01:36.921 --> 00:01:41.025
by a single thread, but executed
by many threads behind the scenes.

00:01:41.102 --> 00:01:44.306
From a user's perspective,
these algorithms appear serial

00:01:44.342 --> 00:01:47.315
since they just call them
like any other function.

00:01:47.363 --> 00:01:49.763
However, under the hood,
they distribute the work

00:01:49.823 --> 00:01:53.696
among multiple threads to
complete the task faster.

00:01:53.804 --> 00:01:55.605
Some examples of
parallel algorithms

00:01:55.625 --> 00:01:57.262
we've used so far include

00:01:57.298 --> 00:02:00.847
Thrust transform and CUB
device transform.

00:02:00.960 --> 00:02:03.945
Now let's look more closely
at cooperative algorithms.

00:02:04.002 --> 00:02:07.964
On the diagram, you can see how
cooperative production might work.

00:02:08.024 --> 00:02:12.210
Here, all threads provide a single
value into cooperative reduction.

00:02:12.286 --> 00:02:16.122
First thread contributes
zero, second one contributes

00:02:16.148 --> 00:02:18.215
one and so forth.

00:02:18.309 --> 00:02:21.829
Inside the cooperative algorithm,
these values are written

00:02:21.851 --> 00:02:23.508
into shared memory.

00:02:23.593 --> 00:02:27.394
After thread block synchronization,
a small set of threads computes

00:02:27.494 --> 00:02:30.576
partial sums and writes them back.

00:02:30.674 --> 00:02:34.335
After one more synchronization,
the total sum, 6 in our

00:02:34.375 --> 00:02:38.540
simplified example, is returned
to the first thread.

00:02:38.616 --> 00:02:41.754
Although this is a very
basic model, it illustrates

00:02:41.796 --> 00:02:44.737
two key points about
cooperative algorithms.

00:02:44.817 --> 00:02:50.782
One, they rely on shared memory
for data exchange between threads.

00:02:50.918 --> 00:02:54.102
They may include
synchronization steps.

00:02:54.180 --> 00:02:57.163
This means if any thread in
the block fails to call the

00:02:57.203 --> 00:03:02.330
cooperative algorithm, the
entire kernel could deadlock.

00:03:02.527 --> 00:03:05.623
Let's see how this intuition
applies to real code.

00:03:05.689 --> 00:03:09.318
Here we have a cooperative
reduction provided by kub.

00:03:09.392 --> 00:03:12.495
Unlike traditional function
oriented interfaces, kub exposes

00:03:12.555 --> 00:03:16.479
its cooperative algorithms
as templated structures.

00:03:16.584 --> 00:03:19.845
Template parameters are used
to specialize algorithms for

00:03:19.885 --> 00:03:21.352
the problem at hand.

00:03:21.406 --> 00:03:25.732
For example, we can specify
that we want to reduce integers

00:03:25.768 --> 00:03:29.129
within a block of 256 threads.

00:03:29.229 --> 00:03:33.271
Next, there's a nested temp
storage type, which specifies

00:03:33.311 --> 00:03:36.742
the kind of temporary storage
the cooperative algorithm needs

00:03:36.772 --> 00:03:38.957
for its internal communication.

00:03:39.033 --> 00:03:42.297
We allocate an instance of
this type in shared memory,

00:03:42.335 --> 00:03:45.664
then pass a reference to that
shared memory instance

00:03:45.714 --> 00:03:49.096
When we construct the
cooperative algorithm.

00:03:49.217 --> 00:03:52.349
Finally, member functions
provide different flavors

00:03:52.380 --> 00:03:55.021
of a given cooperative algorithm.

00:03:55.142 --> 00:03:57.790
Putting it all together,
usage of this cooperative

00:03:57.804 --> 00:03:59.579
algorithm looks like this.

00:03:59.645 --> 00:04:03.489
We instantiate block reduction
for reducing integers within

00:04:03.529 --> 00:04:05.742
a thread block of size four.

00:04:05.831 --> 00:04:10.244
Then allocate nest attempt
storage type in shared memory,

00:04:10.314 --> 00:04:14.007
construct an instance of this
cooperative algorithm structure,

00:04:14.062 --> 00:04:17.709
And finally, call a
cooperative algorithm.

00:04:17.824 --> 00:04:23.056
CUB provides many general purpose
algorithms at a thread block level.

00:04:23.166 --> 00:04:25.848
And one of these cooperative
algorithms is actually

00:04:25.868 --> 00:04:28.494
a block histogram.

00:04:28.649 --> 00:04:32.601
The block histogram structure has
a few template parameters as well.

00:04:32.671 --> 00:04:36.549
First, we have to specify the
type of the histogram elements,

00:04:36.593 --> 00:04:38.764
which is integer in our case.

00:04:38.834 --> 00:04:43.975
Then we have to specify the
thread block size, like 256.

00:04:44.246 --> 00:04:49.155
Then we also specify how many bins
each thread is going to contribute.

00:04:49.209 --> 00:04:53.162
And lastly, we have to specify
how many bins there are

00:04:53.192 --> 00:04:55.136
for our histogram.

00:04:55.253 --> 00:04:58.655
Then the block histogram usage
is very similar to what we

00:04:58.695 --> 00:05:01.280
have seen before.

00:05:01.457 --> 00:05:05.280
In the exercise for this section,
you'll use the CUB block block

00:05:05.320 --> 00:05:09.190
histogram algorithm in the
histogram kernel instead of

00:05:09.202 --> 00:05:10.483
implementing it from scratch.

